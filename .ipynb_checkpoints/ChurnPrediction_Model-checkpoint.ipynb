{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3296de58",
   "metadata": {},
   "source": [
    "# Dataset Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c12ab6f",
   "metadata": {},
   "source": [
    "## Importing Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc43713a",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# importing libraries for data handling and analysis\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from openpyxl import load_workbook\n",
    "from scipy.stats import norm, skew\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6041ae4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries for data visualisations\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "color = sns.color_palette()\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = None\n",
    "# Standard plotly imports\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objects as go\n",
    "import plotly\n",
    "#import plotly.plotly as py\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "# Using plotly + cufflinks in offline mode\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import cufflinks as cf\n",
    "cf.set_config_file(offline=True)\n",
    "import cufflinks\n",
    "cufflinks.go_offline(connected=True)\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8a7a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn modules for preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "# from imblearn.over_sampling import SMOTE  # SMOTE\n",
    "# sklearn modules for ML model selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Libraries for data modelling\n",
    "from sklearn import svm, tree, linear_model, neighbors\n",
    "from sklearn import naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.compat.v1.losses import sparse_softmax_cross_entropy\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import lime\n",
    "import shap\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Common sklearn Model Helpers\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE, VarianceThreshold\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "# from sklearn.datasets import make_classification\n",
    "\n",
    "# sklearn modules for performance metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve\n",
    "from sklearn.metrics import auc, roc_auc_score, roc_curve, recall_score, log_loss\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, make_scorer\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04833a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing misceallenous libraries\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import timeit\n",
    "import string\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "from dateutil.parser import parse\n",
    "# ip = get_ipython()\n",
    "# ip.register_magics(jupyternotify.JupyterNotifyMagics)\n",
    "\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow_tracking.db\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25b008d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ->Load data\n",
    "df = pd.read_csv('SampleDataSet.csv')\n",
    "print(\"Shape of dataframe is: {}\".format(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6202ccef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70568abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the original sourcefile to working sourcefile\n",
    "df_subset = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499d2735",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7e2457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view new or copied Dataset columns\n",
    "df_subset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5990b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the copied or new DataFrame\n",
    "print(df_subset.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3829c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset.columns.to_series().groupby(df.dtypes).groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65399bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns datatypes and missign values\n",
    "df_subset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd98b622",
   "metadata": {},
   "source": [
    "Numerical features overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041489f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb940d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset.hist(figsize=(20,20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56d0b7b",
   "metadata": {},
   "source": [
    "Correlation : Examining some of most significant correlations. note that correlation coefficients only measure linear correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9223d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_subset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d61049c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the working dataset to correlation dataset\n",
    "df_cor = df_subset.copy()\n",
    "\n",
    "# Handle categorical features\n",
    "df_cor['Gender'] = df_cor['Gender'].map({'Female': 0, 'Male': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5937d4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to only numeric columns\n",
    "numeric_cols = df_cor.select_dtypes(include=['number']).columns  \n",
    "df_cor = df_cor[numeric_cols]\n",
    "\n",
    "df_cor[numeric_cols].corrwith(df_cor['Exited'])\n",
    "df_cor.corrwith(df_cor['Exited'], numeric_only=True)\n",
    "print(df_cor.corrwith(df_cor['Exited']).sort_values(ascending=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6159ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(df_cor.select_dtypes(include=['int', 'float']).corr(), annot=True, center=0,cmap='viridis',annot_kws={'size': 12})\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a15929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Test relationship between Balance and HasCrCard \n",
    "corr, p_value = stats.pearsonr(df_cor['Balance'], df_cor['HasCrCard'])  \n",
    "\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print('Significant correlation between Balance and HasCrCard')\n",
    "else:\n",
    "    print('No significant correlation found')\n",
    "    \n",
    "# Can repeat for other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f838ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 6))\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.set_palette(\"Set1\")\n",
    "sns.countplot(x=\"Complain\", data=df_subset,hue='Exited')\n",
    "plt.xlabel(\"Complain\",fontsize=18)\n",
    "plt.ylabel(\"Exited\",fontsize=18)\n",
    "plt.title(\"Complain vs Exited\",fontsize=18)\n",
    "plt.legend(title=\"Exited?\", labels=[\"No\", \"Yes\"],fontsize=16)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bb0f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the columns of interest\n",
    "columna1 = df_subset['Complain']\n",
    "columna2 = df_subset['Exited']\n",
    "\n",
    "# Calculate the values of a, b, and c\n",
    "a = sum((columna1 == 1) & (columna2 == 1))\n",
    "b = sum((columna1 == 1) & (columna2 == 0))\n",
    "c = sum((columna1 == 0) & (columna2 == 1))\n",
    "\n",
    "# Calculate the adjusted Jaccard coefficient\n",
    "adjusted_jaccard = a / (a + b + c)\n",
    "print(f\"Adjusted Jaccard Coefficient: {adjusted_jaccard:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cd4ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(df_subset['CreditScore'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8576b715",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30385a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a label encoder object\n",
    "le = LabelEncoder()\n",
    "encoded_columns = []  # List to store columns that were label encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9ca614",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_subset.shape)\n",
    "df_subset.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f5368f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding will be used for columns with 2 or less unique values\n",
    "le_count = 0\n",
    "for col in df_subset.columns[1:]:\n",
    "    if df_subset[col].dtype == 'object':\n",
    "        if len(list(df_subset[col].unique())) <= 2:\n",
    "            le.fit(df_subset[col])\n",
    "            df_subset[col] = le.transform(df_subset[col])\n",
    "            le_count += 1\n",
    "print('{} columns were label encoded.'.format(le_count))\n",
    "print('Columns label encoded:', encoded_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a19fcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any column name contains 'Unnamed'\n",
    "unnamed_columns = [col for col in df_subset.columns if 'Unnamed' in col]\n",
    "\n",
    "# Print the result\n",
    "if len(unnamed_columns) > 0:\n",
    "    print(f'Columns with \"Unnamed\": {unnamed_columns}')\n",
    "else:\n",
    "    print('No columns with \"Unnamed\" found.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025181fa",
   "metadata": {},
   "source": [
    "# Splitting data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c5e38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_subset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420bd49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into features (X) and target variable (y)\n",
    "X = df_subset.drop(['RowNumber', 'CustomerId', 'Surname', 'Geography', 'Card Type'], axis=1)\n",
    "Y = df_subset['Exited']\n",
    "\n",
    "X.head()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0827aa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split categorical and numerical columns into separate dataframes\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = df_subset.select_dtypes(include=['object']).columns\n",
    "numerical_cols = df_subset.select_dtypes(include=['int32', 'int64', 'float64']).columns\n",
    "\n",
    "# Create dataframes for categorical and numerical columns\n",
    "df_categorical = df_subset[categorical_cols]\n",
    "df_numerical = df_subset[numerical_cols]\n",
    "\n",
    "# Example: Print the first few rows of each dataframe\n",
    "print(\"Categorical Dataframe:\")\n",
    "print(df_categorical.head())\n",
    "\n",
    "print(\"\\nNumerical Dataframe:\")\n",
    "print(df_numerical.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9c5cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "              \n",
    "# Split data into features (X) and target variable (y)\n",
    "X = df_subset.drop(['RowNumber', 'CustomerId', 'Surname', 'Geography', 'Card Type'], axis=1)\n",
    "y = df_subset['Exited']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y) #, \n",
    "\n",
    "# Check the lengths of temporary sets\n",
    "print(\"length of X_temp sets:\", len(X_temp))  \n",
    "print(\"length of y_temp sets:\", len(y_temp))\n",
    "\n",
    "# Split the temporary set into validation and test sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "# Drop 'Churn' column if it exists\n",
    "if 'Exited' in X_train.columns:\n",
    "    X_train = X_train.drop('Exited', axis=1)\n",
    "\n",
    "if 'Exited' in X_test.columns:\n",
    "    X_test = X_test.drop('Exited', axis=1)\n",
    "\n",
    "print(\"X_train\", X_train.dtypes)\n",
    "print(\"X_test\", X_test.dtypes)\n",
    "print(\"X_train columns:\", X_train.columns)\n",
    "print(\"X_test columns:\", X_test.columns)\n",
    "\n",
    "# Check the shapes of the resulting sets\n",
    "print(\"Train Set Shape:     \", X_train.shape)\n",
    "print(\"Validation Set Shape:\", X_val.shape)\n",
    "print(\"Test Set Shape:      \", X_test.shape)\n",
    "print(\"Temporary Set Shape: \", X_temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc70aca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for handling categorical features\n",
    "cat_cols = ['CreditScore', 'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n",
    "            'IsActiveMember', 'EstimatedSalary', 'Complain', 'Satisfaction Score', 'Point Earned']\n",
    "\n",
    "# Step 1: Handle unknown categories in training data\n",
    "for col in cat_cols:\n",
    "    if col in X_train.columns:\n",
    "        X_train[col] = X_train[col].replace(['unknown_category_train'], np.nan)\n",
    "\n",
    "# Step 2: Handle unknown categories in validation data\n",
    "for col in cat_cols:\n",
    "    if col in X_val.columns:\n",
    "        X_val[col] = X_val[col].replace(['unknown_category_val'], np.nan)\n",
    "\n",
    "# Step 3: Handle unknown categories in test data\n",
    "for col in cat_cols:\n",
    "    if col in X_test.columns:\n",
    "        X_test[col] = X_test[col].replace(['unknown_category_test'], np.nan)\n",
    "\n",
    "# Step 4: One-hot encoding\n",
    "X_train_encoded = pd.get_dummies(X_train, columns=cat_cols, dummy_na=True)\n",
    "X_val_encoded = pd.get_dummies(X_val, columns=cat_cols, dummy_na=True)\n",
    "X_test_encoded = pd.get_dummies(X_test, columns=cat_cols, dummy_na=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15635b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the Columns\n",
    "print(\"Columns in the DataFrame:\")\n",
    "print(df_subset.columns)\n",
    "\n",
    "# Check for NaN Values\n",
    "print(\"\\nNaN Values in the DataFrame:\")\n",
    "print(df_subset.isnull().sum())\n",
    "\n",
    "# Check Unique Values\n",
    "print(\"\\nUnique Values in Categorical Columns:\")\n",
    "categorical_cols = df_subset.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    print(f\"Unique values in {col}: {df_subset[col].unique()}\")\n",
    "\n",
    "# Verify Encoding in Original DataFrame\n",
    "print(\"\\nVerification of Encoding in Original DataFrame:\")\n",
    "print(df_subset[cat_cols].head())  # Replace with your categorical columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafd4e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# List of remaining categorical columns\n",
    "remaining_cat_cols = ['RowNumber', 'CustomerId', 'Surname', 'Geography', 'Card Type']\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Step 1: Handle unknown categories in training data\n",
    "for col in remaining_cat_cols:\n",
    "    if col in X_train.columns:\n",
    "        X_train[col] = X_train[col].replace(['unknown_category_train'], np.nan)\n",
    "\n",
    "# Step 2: Handle unknown categories in validation data\n",
    "for col in remaining_cat_cols:\n",
    "    if col in X_val.columns:\n",
    "        X_val[col] = X_val[col].replace(['unknown_category_val'], np.nan)\n",
    "\n",
    "# Step 3: Handle unknown categories in test data\n",
    "for col in remaining_cat_cols:\n",
    "    if col in X_test.columns:\n",
    "        X_test[col] = X_test[col].replace(['unknown_category_test'], np.nan)\n",
    "\n",
    "# Step 4: Apply Label Encoding\n",
    "for col in remaining_cat_cols:\n",
    "    if col in X_train.columns:\n",
    "        X_train[col] = label_encoder.fit_transform(X_train[col].astype(str))\n",
    "\n",
    "for col in remaining_cat_cols:\n",
    "    if col in X_val.columns:\n",
    "        X_val[col] = label_encoder.transform(X_val[col].astype(str))\n",
    "\n",
    "for col in remaining_cat_cols:\n",
    "    if col in X_test.columns:\n",
    "        X_test[col] = label_encoder.transform(X_test[col].astype(str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33927e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Create an imputer instance\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Check missing values before and after imputation\n",
    "print(\"Before imputation:\", X_train.isnull().sum())\n",
    "\n",
    "# Fit and transform the imputer on your training data\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using the same imputer\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "## Scale the Imputed Data: ##\n",
    "\n",
    "# Scale the numerical features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the scaler on the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "\n",
    "# Transform the test data using the same scaler\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "# Convert the scaled data back to a DataFrame\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "\n",
    "# Check missing values after imputation and scaling\n",
    "print(\"After imputation and scaling:\", X_train_scaled_df.isnull().sum())\n",
    "\n",
    "# Check the shapes of your training and testing datasets:\n",
    "print(\"X_train scaled shape: \", X_train_scaled.shape)\n",
    "print(\"y_train shape:        \", y_train.shape)\n",
    "print(\"X_test scaled shape:  \", X_test_scaled.shape)\n",
    "print(\"y_test shape:         \", y_test.shape)\n",
    "\n",
    "# Create a DataFrame after imputation with column names\n",
    "X_train_imputed_df = pd.DataFrame(X_train_imputed, columns=X_train.columns)\n",
    "\n",
    "# Check for missing values after imputation\n",
    "print(\"After imputation:\")\n",
    "print(X_train_imputed_df.isnull().sum())\n",
    "\n",
    "# Check shapes after feature scaling\n",
    "print(\"X_train_scaled shape:\")\n",
    "print(X_train_scaled.shape)\n",
    "print(\"X_test_scaled shape:\")\n",
    "print(X_test_scaled.shape)\n",
    "\n",
    "# Note: The following lines are related to X_train_selected, which is not defined\n",
    "# print(\"X_train_selected shape:\", X_train_selected.shape)\n",
    "# print(\"X_test_selected shape:\", X_test_selected.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5baa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine scaled and imputed features\n",
    "X_train_combined = np.concatenate([X_train_scaled, X_train_imputed], axis=1)\n",
    "\n",
    "# Convert the combined data back to a DataFrame\n",
    "columns_combined = list(X_train.columns) + [f\"{col}_imputed\" for col in X_train.columns]\n",
    "X_train_combined_df = pd.DataFrame(X_train_combined, columns=columns_combined)\n",
    "\n",
    "# Check the shapes of your training datasets:\n",
    "print(\"X_train_combined shape: \", X_train_combined.shape)\n",
    "print(\"X_train_combined_df shape: \", X_train_combined_df.shape)\n",
    "print(\"y_train shape:           \", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b74d5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature Selection - SelectKBest\n",
    "k = 10  # choose the number of top features you want\n",
    "selector_kbest = SelectKBest(f_classif, k=k)\n",
    "X_train_selected_kbest = selector_kbest.fit_transform(X_train_scaled, y_train)\n",
    "X_test_selected_kbest = selector_kbest.transform(X_test_scaled)\n",
    "\n",
    "# Feature Selection - Recursive Feature Elimination (RFE) \n",
    "# Assuming you're using a classifier like RandomForestClassifier\n",
    "estimator = RandomForestClassifier()\n",
    "selector_rfe = RFE(estimator, n_features_to_select=k)\n",
    "X_train_selected_rfe = selector_rfe.fit_transform(X_train_imputed, y_train)\n",
    "X_test_selected_rfe = selector_rfe.transform(X_test_imputed)\n",
    "\n",
    "# Dimensionality Reduction - PCA\n",
    "pca = PCA(n_components=k)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Check shapes after feature selection\n",
    "print(\"X_train_selected_kbest shape:\", X_train_selected_kbest.shape)\n",
    "print(\"X_test_selected_kbest shape:\", X_test_selected_kbest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33695b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Constant Features\n",
    "X_train_selected = X_train.copy()\n",
    "selector = VarianceThreshold()\n",
    "X_train_selected = selector.fit_transform(X_train_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f89e2f",
   "metadata": {},
   "source": [
    "BASELINE\n",
    "1. Trains a set of models like Logistic Regression, Random Forest, SVM etc.\n",
    "2. Applies different feature selection methods for some models (RFE, SelectKBest etc).\n",
    "3. Evaluates each model on train and test sets using classification metrics and cross-validation.\n",
    "4. The approach is more basic, training each model separately without ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b31cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Define a list of models along with their corresponding feature selection methods\n",
    "models = [\n",
    "    ('Logistic Regression', LogisticRegression(), None),\n",
    "    ('Random Forest', RandomForestClassifier(n_estimators=100), SelectKBest(k=10)),  \n",
    "    ('Support Vector Machine', SVC(), RFE(estimator=RandomForestClassifier(), n_features_to_select=10)),\n",
    "    ('PCA + Logistic Regression', LogisticRegression(), PCA(n_components=10)),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(), None),\n",
    "    ('AdaBoost', AdaBoostClassifier(), None),\n",
    "    ('K-Nearest Neighbors', KNeighborsClassifier(), None),\n",
    "    ('Naive Bayes', GaussianNB(), None),\n",
    "    ('Neural Network', MLPClassifier(max_iter=500), None),\n",
    "    ('XGBoost', XGBClassifier(), None),\n",
    "]\n",
    "\n",
    "# Increase the number of iterations for the neural network\n",
    "model_nn = MLPClassifier(learning_rate='adaptive', max_iter=500, tol=1e-4)\n",
    "    \n",
    "# Adjust the smoothing parameter for Gaussian Naive Bayes\n",
    "model_nb = GaussianNB(var_smoothing=1e-3)\n",
    "\n",
    "# Loop over each model\n",
    "for model_name, model, feature_selector in models:\n",
    "    # Apply feature selection if a feature_selector is specified\n",
    "    if feature_selector is not None:\n",
    "        # Apply feature selection and transformation\n",
    "        X_train_selected = feature_selector.fit_transform(X_train_scaled, y_train)\n",
    "        X_test_selected = feature_selector.transform(X_test_scaled)\n",
    "    else:\n",
    "        # If no feature selection is specified, use the original features\n",
    "        X_train_selected = X_train_scaled\n",
    "        X_test_selected = X_test_scaled\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    \n",
    "    # For Neural Network and Naive Bayes, use the pre-defined models\n",
    "    if model_name == 'Neural Network':\n",
    "        model_nn.fit(X_train_selected, y_train)\n",
    "    elif model_name == 'Naive Bayes':\n",
    "        model_nb.fit(X_train_selected, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_train = model.predict(X_train_selected)\n",
    "    y_pred_test = model.predict(X_test_selected)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(\"Training Set:\")\n",
    "    print(classification_report(y_train, y_pred_train))\n",
    "    print(\"Test Set:\")\n",
    "    print(classification_report(y_test, y_pred_test))\n",
    "    \n",
    "    # Out-of-time Cross-Validation\n",
    "    cv_scores = cross_val_score(model, X_train_selected, y_train, cv=5)\n",
    "    print(f\"Cross-Validation Scores: {cv_scores}\")\n",
    "    print(f\"Mean CV Score: {np.mean(cv_scores)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15055831",
   "metadata": {},
   "source": [
    "ML modeling pipeline\n",
    "1. Focuses on creating a stacked ensemble model with base models and meta model\n",
    "2. Tuned Random Forest hyperparameters using GridSearchCV\n",
    "3. Stacks base models like Logistic Regression, Random Forest using meta-learner\n",
    "4. Logs metrics and parameters with MLflow for tracking experiments\n",
    "5. Includes model explanations using LIME and SHAP\n",
    "6. Handles validation set and converting probabilities to metrics\n",
    "7. More productionized approach with MLflow and model diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c075aa42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score,classification_report, roc_curve, precision_recall_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import lime\n",
    "import shap\n",
    "from IPython.display import display\n",
    "\n",
    "class ChurnPredictor:\n",
    "    def __init__(self):\n",
    "        self.models = []\n",
    "        self.stack = None\n",
    "\n",
    "    def add_model(self, model):\n",
    "        self.models.append(model)\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        # Start an MLflow run\n",
    "        with mlflow.start_run():\n",
    "            try:\n",
    "                # Tune RF hyperparameters\n",
    "                rf_grid = {'n_estimators': [100, 200], 'max_depth': [4, 6]}\n",
    "                rf_gs = GridSearchCV(RandomForestClassifier(), rf_grid, cv=5)\n",
    "                rf_gs.fit(X_train, y_train)\n",
    "\n",
    "                self.rf = rf_gs.best_estimator_\n",
    "\n",
    "                # Log RF params with MLflow\n",
    "                mlflow.log_params(self.rf.get_params())\n",
    "                \n",
    "                \n",
    "                # Train base models\n",
    "                for model in self.models:\n",
    "                    model.fit(X_train, y_train)\n",
    "\n",
    "                # Create stacking ensemble\n",
    "                self.stack = StackingClassifier(\n",
    "                    estimators=[(str(i), model) for i, model in enumerate(self.models)],\n",
    "                    final_estimator=LogisticRegression(),\n",
    "                    stack_method='predict_proba'\n",
    "                )\n",
    "\n",
    "                # Create stack ensemble\n",
    "                self.stack.fit(X_train, y_train)\n",
    "                \n",
    "                # Use feature names from X_train.columns\n",
    "                feature_names = X_train.columns\n",
    "\n",
    "                # Log feature names as tags\n",
    "                mlflow.log_param('feature_names', ', '.join(feature_names))\n",
    "\n",
    "                \n",
    "\n",
    "            finally:\n",
    "                # Ensure the MLflow run is always ended\n",
    "                mlflow.end_run()\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        try:\n",
    "            \n",
    "            # Try to get probabilities using predict_proba. Assuming binary classification\n",
    "            proba_scores = self.stack.predict_proba(X_test)\n",
    "            \n",
    "            # Ensure probabilities sum to 1\n",
    "            assert np.allclose(np.sum(proba_scores, axis=1), 1.0), \"Probabilities do not sum to 1\"\n",
    "            \n",
    "            ## Return the probabilitiess\n",
    "            return proba_scores[:, 1]\n",
    "        \n",
    "        except NotImplementedError:\n",
    "            # Handle classifiers without probability scores. Return probability scores\n",
    "            proba_scores = self.stack.predict_proba(X_test)\n",
    "            return proba_scores[:, 1]\n",
    "           \n",
    "    def _validate_input(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            return X\n",
    "        elif isinstance(X, np.ndarray):\n",
    "            # Convert NumPy array to DataFrame, assuming columns are named numerically\n",
    "            return pd.DataFrame(X, columns=[str(i) for i in range(X.shape[1])])\n",
    "        else:\n",
    "            raise ValueError(\"Input must be a DataFrame or a NumPy array.\")\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        with mlflow.start_run():\n",
    "            try:\n",
    "                # Extract feature names\n",
    "                feature_names = X_train.columns\n",
    "\n",
    "                # Check if X_test is a DataFrame\n",
    "                X_test_df = self._validate_input(X_test)\n",
    "\n",
    "                if y_test is None:\n",
    "                    raise ValueError(\"y_test cannot be None.\")\n",
    "\n",
    "                # Get predictions\n",
    "                predictions = self.predict(X_test)\n",
    "                # Convert probabilities to binary predictions\n",
    "                binary_predictions = (predictions > 0.5).astype(int)\n",
    "                # Convert y_test to binary labels\n",
    "                binary_labels = (y_test > 0.5).astype(int)\n",
    "                # Convert pandas Series to NumPy array\n",
    "                binary_labels = binary_labels.values\n",
    "                \n",
    "                # Model evaluation\n",
    "                print(classification_report(binary_labels, binary_predictions))\n",
    "                \n",
    "                print(type(binary_predictions))\n",
    "                print(type(binary_labels))\n",
    "                print(binary_predictions.shape)\n",
    "                print(binary_labels.shape)\n",
    "\n",
    "                # Model evaluation\n",
    "                #print(classification_report(y_test, predictions))\n",
    "\n",
    "                # Log metrics with MLflow\n",
    "                mlflow.log_metric(\"test_accuracy\", accuracy_score(binary_labels, binary_predictions))\n",
    "                mlflow.log_metric(\"test_roc_auc\", roc_auc_score(binary_labels, predictions))\n",
    "                \n",
    "                \n",
    "\n",
    "                # Print and display feature_names\n",
    "                print(\"Feature Names:\", feature_names)\n",
    "                display(feature_names)\n",
    "\n",
    "                # Choose the number of features for explanation (num_features)\n",
    "                num_features = 10\n",
    "                \n",
    "                X_test_df = pd.DataFrame(X_test, columns=feature_names)\n",
    "\n",
    "                # Explainability\n",
    "                #explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "                    #X_test_df.values,\n",
    "                    #feature_names=feature_names,\n",
    "                    #class_names=[\"Negative\", \"Positive\"],\n",
    "                    #discretize_continuous=True\n",
    "                #)\n",
    "                #exp = explainer.explain_instance(X_test_df.values[0], self.predict, num_features=10)\n",
    "                #exp.show_in_notebook()\n",
    "\n",
    "                # Shap\n",
    "                background = shap.sample(X_test_df, 100)\n",
    "                explainer = shap.KernelExplainer(self.stack.predict, background)\n",
    "                shap_values = explainer.shap_values(X_test_df)\n",
    "                shap.summary_plot(shap_values, X_test_df)\n",
    "\n",
    "                return predictions\n",
    "            \n",
    "            finally:\n",
    "                # Ensure the MLflow run is always ended\n",
    "                mlflow.end_run()\n",
    "\n",
    "    def evaluate_cv(self, X, y, cv=5):\n",
    "        with mlflow.start_run():\n",
    "            try:\n",
    "                # Use cross_val_score for cross-validated evaluation\n",
    "                scores = cross_val_score(self.stack, X, y, cv=cv, scoring='accuracy')\n",
    "\n",
    "                # Log the average and standard deviation of accuracy across folds\n",
    "                mlflow.log_metric(\"cv_accuracy_mean\", scores.mean())\n",
    "                mlflow.log_metric(\"cv_accuracy_std\", scores.std())\n",
    "\n",
    "            finally:\n",
    "                # Ensure the MLflow run is always ended\n",
    "                mlflow.end_run()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83514268",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = ChurnPredictor()\n",
    "\n",
    "# Add models\n",
    "predictor.add_model(LogisticRegression(max_iter=1000))\n",
    "predictor.add_model(RandomForestClassifier())\n",
    "\n",
    "# Fit and evaluate\n",
    "predictor.fit(X_train, y_train)\n",
    "predictions = predictor.evaluate(X_test, y_test)\n",
    "predictor.evaluate_cv(X_train, y_train)\n",
    "\n",
    "# Model evaluation\n",
    "#print(classification_report(y_test, predictions))\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, predictions)\n",
    "plt.plot(fpr, tpr)\n",
    "plt.title('ROC Curve')\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision, recall, _ = precision_recall_curve(y_test, predictions) \n",
    "plt.plot(recall, precision)\n",
    "\n",
    "# Cross-validation\n",
    "print(\"cross_val_score:\")\n",
    "print(cross_val_score(model, X, y, cv=5))\n",
    "\n",
    "# Log metrics\n",
    "mlflow.log_metric(\"test_roc_auc\", roc_auc_score(y_test, predictions))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270dc3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test.unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18cb9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# customize model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class RNNModule(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNModule, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first =True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size, 32) #output size 32\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "# this create an instance of RNNModel\n",
    "#input_size = 1000  # Adjust the input size based on your data\n",
    "#hidden_size = 32  # Adjust the hidden size based on your preference\n",
    "#output_size = 32   # Adjust the output size based on your task\n",
    "#rn_module = rn_module(input_size, hidden_size, output_size)\n",
    "        \n",
    "class GraphConv(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(GraphConv, self).__init__()\n",
    "        # Define your graph convolution layer parameters here\n",
    "\n",
    "    def forward(self, adj, features):\n",
    "        # Implement the forward pass of your graph convolution layer\n",
    "        # The input 'adj' is the adjacency matrix and 'features' are node features\n",
    "        # implementation will depend on the specific type of graph convolution used\n",
    "\n",
    "        # Assuming a basic graph convolution operation\n",
    "        output = torch.matmul(adj, features)  # Adjust this based on the actual graph convolution operation\n",
    "\n",
    "        return output\n",
    "\n",
    "class GNNModule(nn.Module):\n",
    "    def __init__(self, input_features, hidden_features, output_features):\n",
    "        super(GNNModule, self).__init__()\n",
    "        \n",
    "        # Assuming GraphConv is a custom graph convolution layer\n",
    "        self.gc = GraphConv(input_features, output_features)\n",
    "        self.fc = nn.Linear(output_features, output_features)  # Adjust the linear layer based on your needs\n",
    "\n",
    "    def forward(self, adj, features):\n",
    "        # Apply graph convolution\n",
    "        gc_output = self.gc(adj, features)\n",
    "\n",
    "        # Apply a linear layer or any other operations based on your architecture\n",
    "        output = self.fc(gc_output)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Now you can create an instance of GNNModule\n",
    "#input_features = 64  # Adjust based on your graph node features\n",
    "#hidden_features = 32  # Adjust based on your preference\n",
    "#output_features = 16  # Adjust based on your needs\n",
    "#gnn_module(input_features=64, hidden_features=32,output_features=16)\n",
    "        \n",
    "class NCFModule(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NCFModule, self).__init__()\n",
    "\n",
    "        # Embedding layers for user and item\n",
    "        self.user_embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.item_embedding = nn.Embedding(input_size, hidden_size)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(2 * hidden_size, 64)  # Concatenate user and item embeddings\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, output_size)\n",
    "\n",
    "        # Activation function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, user_ids, item_ids):\n",
    "        # Embed user and item IDs\n",
    "        user_embedded = self.user_embedding(user_ids)\n",
    "        item_embedded = self.item_embedding(item_ids)\n",
    "\n",
    "        # Concatenate user and item embeddings\n",
    "        concatenated = torch.cat([user_embedded, item_embedded], dim=1)\n",
    "\n",
    "        # Forward pass through fully connected layers\n",
    "        x = self.relu(self.fc1(concatenated))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        output = self.fc3(x)\n",
    "\n",
    "        return output\n",
    "\n",
    "# usage\n",
    "#input_size = 1000  # Adjust based on the number of unique users/items in your dataset\n",
    "#hidden_size = 50  # Adjust based on your preference\n",
    "#output_size = 32  # Output size, e.g., for binary classification\n",
    "#ncf_module = ncf_module(input_size, hidden_size, output_size)\n",
    "\n",
    "class TNRFinalModel(nn.Module):\n",
    "    def __init__(self, rn_input_size, gnn_input_features, ncf_input_size, hidden_size, output_size):\n",
    "        super(TNRFinalModel, self).__init__()\n",
    "\n",
    "        self.rnn_module = RNNModule(rn_input_size, hidden_size, output_size)\n",
    "        self.gnn_module = GNNModule(gnn_input_features, hidden_size, output_size)\n",
    "        self.ncf_module = NCFModule(ncf_input_size, hidden_size, output_size)\n",
    "\n",
    "    def forward(self, rnn_input, gnn_adj, gnn_features, ncf_user_ids, ncf_item_ids):\n",
    "        rnn_output = self.rnn_module(rnn_input)\n",
    "        gnn_output = self.gnn_module(gnn_adj, gnn_features)\n",
    "        ncf_output = self.ncf_module(ncf_user_ids, ncf_item_ids)\n",
    "\n",
    "        # Combine the outputs of the three modules as needed\n",
    "        TNRFinalModel_output = torch.cat([rnn_output, gnn_output, ncf_output], dim=1)\n",
    "\n",
    "        return TNRFinalModel_output\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8160100d",
   "metadata": {},
   "source": [
    "class tnr_final_model(nn.Module):\n",
    "    def __init__(self, rn_input_size, gnn_input_features, ncf_input_size, hidden_size, output_size):\n",
    "        super(tnr_final_model, self).__init__()\n",
    "\n",
    "        self.rnn_module = RNNModule(rn_input_size, hidden_size, output_size)  # Corrected here\n",
    "        self.gnn_module = GNNModule(gnn_input_features, hidden_size, output_features=16)\n",
    "        self.ncf_module = NCFModule(ncf_input_size, hidden_size, output_size=16)\n",
    "\n",
    "        self.layer1 = nn.Linear(rn_input_size + gnn_input_features + ncf_input_size, 32)\n",
    "        self.layer2 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x, adj, features, user_ids, item_ids):\n",
    "        rnn_out = self.rnn_module(x)\n",
    "        gnn_out = self.gnn_module(adj, features)\n",
    "        ncf_out = self.ncf_module(user_ids, item_ids)\n",
    "\n",
    "        concat = torch.cat([rnn_out, gnn_out, ncf_out], dim=1)\n",
    "        x = F.relu(self.layer1(concat))  # Apply ReLU activation\n",
    "        x = torch.sigmoid(self.layer2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434882f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Model training hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "\n",
    "rn_input_size = 1000  \n",
    "gnn_input_size = 64\n",
    "ncf_input_size = 1000\n",
    "hidden_size = 50\n",
    "output_size = 1\n",
    "\n",
    "# Instantiate model\n",
    "model = TNRFinalModel(rn_input_size, gnn_input_size, ncf_input_size, hidden_size, output_size)\n",
    "\n",
    "# Get model parameters\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)  \n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# Extract input tensors from preprocessed data\n",
    "X_seq = rnn_sequence_data\n",
    "A_train = adjacency_matrix\n",
    "X_feat = customer_features\n",
    "user_ids = user_ids_data\n",
    "item_ids = item_ids_data\n",
    "y_train = churn_labels\n",
    "\n",
    "dataset = TensorDataset(X_seq, A_train, X_feat, user_ids, item_ids, y_train)\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for seq_batch, adj_batch, feat_batch, user_batch, item_batch, label_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(seq_batch, adj_batch, feat_batch, user_batch, item_batch)\n",
    "        loss = loss_fn(output, label_batch.unsqueeze(1).float())  # Ensure labels are in the correct format\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1} | Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c983dce4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da1fd032",
   "metadata": {},
   "source": [
    "# Usage example\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Assuming X_train, y_train, X_test, y_test are defined\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Handle unknown categories in training data\n",
    "for col in cat_cols:\n",
    "    X_train[col] = X_train[col].replace(['unknown_category_train'], np.nan)\n",
    "\n",
    "# Handle unknown categories in test data\n",
    "for col in cat_cols:\n",
    "    X_test[col] = X_test[col].replace(['unknown_category_test'], np.nan)\n",
    "\n",
    "# Create a ColumnTransformer to apply OneHotEncoder to categorical columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(), cat_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_encoded = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_encoded = preprocessor.transform(X_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
